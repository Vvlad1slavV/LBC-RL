{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e3136-25f7-4570-9c88-319fc7b1a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch as th\n",
    "from tqdm import tqdm\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "from imitation.algorithms import bc\n",
    "\n",
    "env_id = \"MountainCarContinuous-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4cdcc-4478-4fe8-985f-a5d438c2d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MountainCar_utils.observation_wrapper import MountainCarContinuousObsWrapper, MountainCarContinuousNoVelObsWrapper\n",
    "\n",
    "def wrapper(env):\n",
    "    env = MountainCarContinuousObsWrapper(env) \n",
    "    return env\n",
    "\n",
    "def wrapper_no_vel(env):\n",
    "    env = MountainCarContinuousNoVelObsWrapper(env) \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee73493-b8d3-4d0b-b1da-acf39a0446b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rew_and_std(rewards):\n",
    "    rewards = np.array(rewards)\n",
    "    sort_rewards = rewards[np.argsort(rewards[:, 0])][2:-2]\n",
    "    rew = np.mean(sort_rewards, axis=0)[0]\n",
    "    std = np.linalg.norm(sort_rewards[:, 1])/np.sqrt(len(sort_rewards))\n",
    "    return rew, std\n",
    "\n",
    "def get_best_result(rewards):\n",
    "    rewards = np.array(rewards)\n",
    "    sort_rewards = rewards[np.argsort(rewards[:, 0])]\n",
    "    return sort_rewards[-1]\n",
    "\n",
    "def print_rew_std(label: str, expert_rew: float, expert_std: float, latex: bool = True, line_lenght: int = 25):\n",
    "    if latex:\n",
    "        return f'{label:<{line_lenght}} & ${expert_rew:.2f} \\pm {expert_std:.2f}$ \\\\\\ \\hline'\n",
    "    else:\n",
    "        return f'{label:<{line_lenght}} {expert_rew:.2f} +/- {expert_std:.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bba11-7888-4f47-ac68-c8a2ce1e8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_angle = make_vec_env(env_id, wrapper_class=wrapper, n_envs=10)\n",
    "env_no_vel = make_vec_env(env_id, wrapper_class=wrapper_no_vel, n_envs=10)\n",
    "env_no_vel_fs2 = VecFrameStack(env_no_vel, 2)\n",
    "env_no_vel_fs4 = VecFrameStack(env_no_vel, 4)\n",
    "env = make_vec_env(env_id, n_envs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03e4d2-76e1-44a1-8a38-c61a098bcce5",
   "metadata": {},
   "source": [
    "# Full Obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd5ac5-14ec-4e6c-a3ed-702d9b643d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_angle_policy = []\n",
    "for file in glob.glob('./Compare/expert_eval2/expert*.zip'):\n",
    "    expert_angle_policy.append(PPO.load(file, print_system_info=False))    \n",
    "    \n",
    "expert_angle_policy_3264 = []\n",
    "for file in glob.glob('./Compare/expert_3264_eval2/expert*.zip'):\n",
    "    expert_angle_policy_3264.append(PPO.load(file, print_system_info=False))    \n",
    "    \n",
    "noob_policy = []\n",
    "for file in glob.glob('./Compare/noob_eval2/noob*.zip'):\n",
    "    noob_policy.append(PPO.load(file, print_system_info=False))\n",
    "\n",
    "noob_policy_3264 = []\n",
    "for file in glob.glob('./Compare/noob_3264_eval2/noob*.zip'):\n",
    "    noob_policy_3264.append(PPO.load(file, print_system_info=False))\n",
    "\n",
    "bc_policy = th.load('./policy/bc_expert3464.zip')\n",
    "bc_policy_3264 = th.load('./policy/bc_expert46304_n32b64.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d293d-88f0-45f7-b389-a043bd7d1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_angle_policy = []\n",
    "# for file in glob.glob('./Compare/expert_eval2/logs/best_model/expert*/best_model.zip'):\n",
    "#     expert_angle_policy.append(PPO.load(file, print_system_info=False))  \n",
    "    \n",
    "# expert_angle_policy_3264 = []\n",
    "# for file in glob.glob('./Compare/expert_3264_eval2/logs/best_model/expert*/best_model.zip'):\n",
    "#     expert_angle_policy_3264.append(PPO.load(file, print_system_info=False))\n",
    "\n",
    "# noob_policy = []\n",
    "# for file in glob.glob('./Compare/noob_eval2/logs/best_model/noob*/best_model.zip'):\n",
    "#     noob_policy.append(PPO.load(file, print_system_info=False))\n",
    "\n",
    "# noob_policy_3264 = []\n",
    "# for file in glob.glob('./Compare/noob_3264_eval2/logs/best_model/noob*/best_model.zip'):\n",
    "#     noob_policy_3264.append(PPO.load(file, print_system_info=False))\n",
    "\n",
    "# # bc_policy = th.load('./policy/bc1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678d732-cf34-44fe-82b2-3d88d9153633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "NUM_EP = 1000\n",
    "\n",
    "expert_rewards = []\n",
    "for expert in tqdm(expert_angle_policy):\n",
    "    env_angle.seed(SEED)\n",
    "    reward, reward_std = evaluate_policy(expert, env_angle, NUM_EP)\n",
    "    expert_rewards.append([reward, reward_std]) # [] -> ()\n",
    "\n",
    "expert_rewards_3264 = []\n",
    "for expert in tqdm(expert_angle_policy_3264):\n",
    "    env_angle.seed(SEED)\n",
    "    reward, reward_std = evaluate_policy(expert, env_angle, NUM_EP)\n",
    "    expert_rewards_3264.append([reward, reward_std])    \n",
    "\n",
    "noob_rewards = []\n",
    "for noob in tqdm(noob_policy):\n",
    "    env.seed(SEED)\n",
    "    reward, reward_std = evaluate_policy(noob, env, NUM_EP)\n",
    "    noob_rewards.append([reward, reward_std])\n",
    "\n",
    "noob_rewards_3264 = []\n",
    "for noob in tqdm(noob_policy_3264):\n",
    "    env.seed(SEED)\n",
    "    reward, reward_std = evaluate_policy(noob, env, NUM_EP)\n",
    "    noob_rewards_3264.append([reward, reward_std])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb8333d-d45b-40e8-b80f-9d7d74fdb2f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.seed(SEED)\n",
    "bc_rew, bc_std = evaluate_policy(bc_policy, env, NUM_EP)\n",
    "\n",
    "env.seed(SEED)\n",
    "bc_rew_n32b64, bc_std_n32b64 = evaluate_policy(bc_policy_3264, env, NUM_EP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc2fcc-b684-43f2-a36f-299f8fc612fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_rew, expert_std = filter_rew_and_std(expert_rewards)\n",
    "expert_rew_n32b64, expert_std_n32b64 = filter_rew_and_std(expert_rewards_3264)\n",
    "noob_rew, noob_std = filter_rew_and_std(noob_rewards)\n",
    "noob_rew_n32b64, noob_std_n32b64 = filter_rew_and_std(noob_rewards_3264)\n",
    "\n",
    "best_expert_rew, best_expert_std = get_best_result(expert_rewards)\n",
    "best_expert_rew_n32b64, best_expert_std_n32b64 = get_best_result(expert_rewards_3264)\n",
    "best_noob_rew, best_noob_std = get_best_result(noob_rewards)\n",
    "best_noob_rew_n32b64, best_noob_std_n32b64 = get_best_result(noob_rewards_3264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716f703-8b64-42f9-90d9-93b7c1c1c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_lenght = 25\n",
    "# Add Padndas and use pandas.DataFrame.to_latex\n",
    "# print(f'1 & {\"expert reward\":<{line_lenght}} & {expert_rew:.2f} {pm} {expert_std:.2f} \\\\\\ \\hline')\n",
    "print(f'1 & {print_rew_std(\"expert reward\", expert_rew, expert_std)}')\n",
    "print(f'2 & {print_rew_std(\"best expert reward\", best_expert_rew, best_expert_std)}')\n",
    "print(f'3 & {print_rew_std(\"noob reward\", noob_rew, noob_std)}')\n",
    "print(f'4 & {print_rew_std(\"best noob reward\", best_noob_rew, best_noob_std)}')\n",
    "print(f'5 & {print_rew_std(\"BC reward\", bc_rew, bc_std)}')\n",
    "print('n32b64')\n",
    "print(f'1 & {print_rew_std(\"expert reward\", expert_rew_n32b64, expert_std_n32b64)}')\n",
    "print(f'2 & {print_rew_std(\"best expert reward\", best_expert_rew_n32b64, best_expert_std_n32b64)}')\n",
    "print(f'3 & {print_rew_std(\"noob reward\", noob_rew_n32b64, noob_std_n32b64)}')\n",
    "print(f'4 & {print_rew_std(\"best noob reward\", best_noob_rew_n32b64, best_noob_std_n32b64)}')\n",
    "print(f'5 & {print_rew_std(\"BC reward\", bc_rew_n32b64, bc_std_n32b64)}')\n",
    "# print('Imitation Learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af718c-cfb4-4076-a6a9-fbc37b03078f",
   "metadata": {},
   "source": [
    "# NoVel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738f987-b5af-49d5-a74f-34742c787f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "noob_tcn = []\n",
    "for file in glob.glob('./Compare/noob_tcn_3264_eval2/noob*.zip'):\n",
    "    noob_tcn.append(PPO.load(file, print_system_info=False))\n",
    "\n",
    "noob_fs2 = []\n",
    "for file in glob.glob('./Compare/noob_novel_3264_fs2_eval2/noob*.zip'):\n",
    "    noob_fs2.append(PPO.load(file, print_system_info=False))\n",
    "    \n",
    "noob_fs4 = []\n",
    "for file in glob.glob('./Compare/noob_novel_3264_fs4_eval2/noob*.zip'):\n",
    "    noob_fs4.append(PPO.load(file, print_system_info=False))\n",
    "\n",
    "bc_tcn_policy = th.load('./policy/bc_tcn_expert46304_n32b64.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172af3df-6254-4abb-a067-371261bf910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "NUM_EP = 1000\n",
    "\n",
    "noob_tcn_rewards = []\n",
    "for noob in tqdm(noob_tcn):\n",
    "    env_no_vel_fs4.seed(SEED)\n",
    "    reward, reward_std = evaluate_policy(noob, env_no_vel_fs4, NUM_EP)\n",
    "    noob_tcn_rewards.append([reward, reward_std])\n",
    "    \n",
    "noob_fs2_rewards = []\n",
    "for noob in tqdm(noob_fs2):\n",
    "    env_no_vel_fs2.seed(SEED)\n",
    "    reward, reward_std = evaluate_policy(noob, env_no_vel_fs2, NUM_EP)\n",
    "    noob_fs2_rewards.append([reward, reward_std])\n",
    "\n",
    "noob_fs4_rewards = []\n",
    "for noob in tqdm(noob_fs4):\n",
    "    env_no_vel_fs4.seed(SEED)\n",
    "    reward, reward_std = evaluate_policy(noob, env_no_vel_fs4, NUM_EP)\n",
    "    noob_fs4_rewards.append([reward, reward_std])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffeb9e8-1351-4491-8a6b-4b77cc29053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_no_vel_fs4.seed(SEED)\n",
    "bc_tcn_rew, bc_tcn_std = evaluate_policy(bc_tcn_policy, env_no_vel_fs4, NUM_EP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3d2f9-fcd3-4b90-96b7-9ca4e3823c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "noob_tcn_rew, noob_tcn_std = filter_rew_and_std(noob_tcn_rewards)\n",
    "noob_fs2_rew, noob_fs2_std = filter_rew_and_std(noob_fs2_rewards)\n",
    "noob_fs4_rew, noob_fs4_std = filter_rew_and_std(noob_fs4_rewards)\n",
    "\n",
    "best_noob_tcn_rew, best_noob_tcn_std = get_best_result(noob_tcn_rewards)\n",
    "best_noob_fs2_rew, best_noob_fs2_std = get_best_result(noob_fs2_rewards)\n",
    "best_noob_fs4_rew, best_noob_fs4_std = get_best_result(noob_fs4_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81dcd48-40ab-40b7-89ee-4c9104ed4416",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ce16a-660d-4cac-8a78-52d746ed77e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_lenght = 25\n",
    "# pm = '+/-'\n",
    "pm = '\\pm'\n",
    "# Add Padndas\n",
    "print(f'1 & {print_rew_std(\"expert reward\", expert_rew_n32b64, expert_std_n32b64)}')\n",
    "print(f'2 & {print_rew_std(\"best expert reward\", best_expert_rew_n32b64, best_expert_std_n32b64)}')\n",
    "print(f'3 & {print_rew_std(\"noob TCN reward\", noob_tcn_rew, noob_tcn_std)}')\n",
    "print(f'4 & {print_rew_std(\"best noob TCN reward\", best_noob_tcn_rew, best_noob_tcn_std)}')\n",
    "# print(f'5 & {print_rew_std(\"noob fs2 reward\", noob_fs2_rew, noob_fs2_std)}')\n",
    "# print(f'6 & {print_rew_std(\"best noob fs2 reward\", best_noob_fs2_rew, best_noob_fs2_std)}')\n",
    "print(f'5 & {print_rew_std(\"noob fs4 reward\", noob_fs4_rew, noob_fs4_std)}')\n",
    "print(f'6 & {print_rew_std(\"best noob fs4 reward\", best_noob_fs4_rew, best_noob_fs4_std)}')\n",
    "print(f'7 & {print_rew_std(\"BC TCN\", bc_tcn_rew, bc_tcn_std)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1d026-0451-4f96-9886-139a9afac5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "rl-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
