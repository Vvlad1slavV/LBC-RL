{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import gymnasium as gym\n",
    "# sys.modules[\"gym\"] = gym\n",
    "import gym\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gnwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"MountainCarContinuous-v0\"\n",
    "NUM_CPU = 32  # Number of processes to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Observ Wrapper\n",
    "```\n",
    "class MountainCarContinuousObsWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, num_obs_points: int = 6):\n",
    "        super().__init__(env)\n",
    "        self.max_angle = 1\n",
    "        \n",
    "        self.low_state = np.append(self.low_state, -self.max_angle)\n",
    "        self.high_state = np.append(self.high_state, self.max_angle)\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=self.low_state, high=self.high_state, dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return np.append(obs, np.cos(3 * obs[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MountainCar_utils import MountainCarContinuousObsWrapper\n",
    "def wrapper(env):\n",
    "    env = MountainCarContinuousObsWrapper(env) \n",
    "    env = gnwrapper.Animation(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training 'expert' and 'noob' policy\n",
    "Skip this if you have trained policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parallel custom obs environments\n",
    "train_env_f = make_vec_env(env_id, \n",
    "                           n_envs=NUM_CPU,\n",
    "                           wrapper_class=MountainCarContinuousObsWrapper)\n",
    "train_env_f.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(train_env_f, \n",
    "                             best_model_save_path=\"./logs/best_model/expert\",\n",
    "                             log_path=\"./logs/results\",\n",
    "                             eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            train_env_f,\n",
    "            verbose=1, \n",
    "            seed=0,\n",
    "            batch_size=512,\n",
    "            # ent_coef=0.00429,\n",
    "            learning_rate=7.77e-05,\n",
    "            n_epochs=10,\n",
    "            n_steps=8*NUM_CPU,\n",
    "            gae_lambda=0.9,\n",
    "            gamma=0.9999,\n",
    "            # clip_range=0.1,\n",
    "            max_grad_norm=5,\n",
    "            # vf_coef=0.19,\n",
    "            use_sde=True,\n",
    "            # policy_kwargs=dict(log_std_init=-3.29, ortho_init=False),\n",
    "            tensorboard_log=\"./logs/ppo_MountainCar_tensorboard_expert/\")\n",
    "\n",
    "model.learn(total_timesteps=1_000_000, callback=eval_callback, progress_bar=True)\n",
    "model.save(\"./logs/ppo_MountainCar_expert\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel environments\n",
    "train_env = make_vec_env(env_id, \n",
    "                         n_envs=NUM_CPU)\n",
    "train_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(train_env, \n",
    "                             best_model_save_path=\"./logs/best_model/expert_noob\",\n",
    "                             log_path=\"./logs/results\",\n",
    "                             eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            train_env,\n",
    "            verbose=1, \n",
    "            seed=0,\n",
    "            batch_size=512,\n",
    "            # ent_coef=0.00429,\n",
    "            learning_rate=7.77e-05,\n",
    "            n_epochs=10,\n",
    "            n_steps=8*NUM_CPU,\n",
    "            gae_lambda=0.9,\n",
    "            gamma=0.9999,\n",
    "            # clip_range=0.1,\n",
    "            max_grad_norm=5,\n",
    "            # vf_coef=0.19,\n",
    "            use_sde=True,\n",
    "            # policy_kwargs=dict(log_std_init=-3.29, ortho_init=False),\n",
    "            tensorboard_log=\"./logs/ppo_MountainCar_tensorboard_noob/\")\n",
    "\n",
    "model.learn(total_timesteps=1_000_000, callback=eval_callback, progress_bar=True)\n",
    "model.save(\"./logs/ppo_MountainCar_noob\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visual testing of policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_env_f = make_vec_env(env_id,\n",
    "                        wrapper_class=wrapper,\n",
    "                        n_envs=1)\n",
    "\n",
    "expert = PPO.load(\"./logs/ppo_MountainCar_expert.zip\", print_system_info=True)\n",
    "\n",
    "obs = test_env_f.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = expert.predict(obs)\n",
    "    obs, rewards, dones, info = test_env_f.step(action)\n",
    "    test_env_f.render(mode=\"rgb_array\")\n",
    "\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "test_env_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = make_vec_env(env_id,\n",
    "                        wrapper_class=gnwrapper.Animation,\n",
    "                        n_envs=1)\n",
    "\n",
    "noob = PPO.load(\"./logs/ppo_MountainCar_noob.zip\", print_system_info=True)\n",
    "\n",
    "obs = test_env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = noob.predict(obs)\n",
    "    obs, rewards, dones, info = test_env.step(action)\n",
    "    test_env.render(mode=\"rgb_array\")\n",
    "\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imitation Learning\n",
    "In this section used Behavioral Cloning (BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "import dataclasses\n",
    "NUM_EPISODES = 100\n",
    "# expert = PPO.load(\"./logs/ppo_MountainCar_expert.zip\")\n",
    "\n",
    "env = gym.make(env_id)\n",
    "env = RolloutInfoWrapper(env) # Wrapper to save origin obs\n",
    "env = MountainCarContinuousObsWrapper(env) # Wrapper to add angle to obs\n",
    "env = DummyVecEnv([lambda: env]) # Vectorized env\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "rollouts = rollout.rollout(\n",
    "    expert,\n",
    "    env,\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=NUM_EPISODES),\n",
    "    rng=rng,\n",
    "    unwrap=True,\n",
    ")\n",
    "\n",
    "transitions = rollout.flatten_trajectories(rollouts)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"The `rollout` function generated a list of {len(rollouts)} {type(rollouts[0])}.\n",
    "After flattening, this list is turned into a {type(transitions)} object containing {len(transitions)} transitions.\n",
    "The transitions object contains arrays for: {', '.join(transitions.__dict__.keys())}.\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "env = gym.make(env_id)\n",
    "env = gnwrapper.Animation(env)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    "    policy=ActorCriticPolicy(observation_space=env.observation_space,\n",
    "                             action_space=env.action_space,\n",
    "                             lr_schedule=lambda _: torch.finfo(torch.float32).max,\n",
    "                             net_arch=[64, 64]\n",
    "                             )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bc_trainer.train(n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = PPO.load(\"./logs/ppo_MountainCar_expert.zip\")\n",
    "noob = PPO.load(\"./logs/ppo_MountainCar_noob.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env_f = make_vec_env(env_id, wrapper_class=wrapper, n_envs=1)\n",
    "test_env = make_vec_env(env_id, n_envs=1)\n",
    "\n",
    "expert_reward, expert_reward_std = evaluate_policy(expert, test_env_f, 100)\n",
    "bc_reward, bc_reward_std = evaluate_policy(bc_trainer.policy, test_env, 100)\n",
    "noob_reward, noob_reward_std = evaluate_policy(noob, test_env, 100)\n",
    "print(f'expert reward {expert_reward} +/- {expert_reward_std}')\n",
    "print(f'BC reward {bc_reward} +/- {bc_reward_std}')\n",
    "print(f'noob reward {noob_reward} +/- {noob_reward_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare policy architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expert.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noob.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer.policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "rl-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
