{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"MountainCarContinuous-v0\"\n",
    "NUM_CPU = 8  # Number of processes to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./logs/ppo_MountainCar_tensorboard/PPO_1\n",
      "Eval num_timesteps=2000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.3e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.3e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.3e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.3e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1324 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 999        |\n",
      "|    mean_reward          | -0.00676   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04342763 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | -0.0164    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00401   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.827      |\n",
      "|    value_loss           | 0.0285     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00676 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00675 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00675 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 921   |\n",
      "|    iterations      | 2     |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 999        |\n",
      "|    mean_reward          | -0.0126    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04520502 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | -0.0111    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.68       |\n",
      "|    value_loss           | 0.0157     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.013   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.0127  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 838   |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.0139     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041126605 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.878      |\n",
      "|    explained_variance   | 0.00611     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0468     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    std                  | 0.56        |\n",
      "|    value_loss           | 0.0121      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.0139  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 798   |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.00888    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040185172 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00286     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0509     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    std                  | 0.463       |\n",
      "|    value_loss           | 0.00755     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00888 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00888 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00888 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 781   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.00215    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039179407 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | -0.00232    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0384     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.036      |\n",
      "|    std                  | 0.384       |\n",
      "|    value_loss           | 0.00597     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00216 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00205 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00212 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 768   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.00524    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038602248 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.00232     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0761     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    std                  | 0.319       |\n",
      "|    value_loss           | 0.00291     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00523 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00417 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00461 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 763   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.0043     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038682338 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.13       |\n",
      "|    explained_variance   | -0.000739   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0799     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.265       |\n",
      "|    value_loss           | 0.00182     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00403 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00385 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00347 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 751   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 87    |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.00548    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039032213 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0514      |\n",
      "|    explained_variance   | -0.0132     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0453     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.22        |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00465 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00536 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00562 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 739   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000392   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037756965 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.238       |\n",
      "|    explained_variance   | 0.00205     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0524     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.000663    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000511 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 76000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000506 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 78000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000492 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 80000     |\n",
      "----------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 730   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 112   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000193   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038376786 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.412       |\n",
      "|    explained_variance   | -0.011      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0157     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.152       |\n",
      "|    value_loss           | 0.000388    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000139 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 84000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000226 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 86000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000173 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 88000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000238 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 90000     |\n",
      "----------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 715   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 125   |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 999        |\n",
      "|    mean_reward          | -0.00184   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 92000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03598807 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.598      |\n",
      "|    explained_variance   | 0.00359    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000285   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00287 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00121 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00129 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 706   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 139   |\n",
      "|    total_timesteps | 98304 |\n",
      "------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 999        |\n",
      "|    mean_reward          | -0.00148   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03772846 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.77       |\n",
      "|    explained_variance   | -0.00322   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.106      |\n",
      "|    value_loss           | 0.000184   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00234 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00123 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00187 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 702    |\n",
      "|    iterations      | 13     |\n",
      "|    time_elapsed    | 151    |\n",
      "|    total_timesteps | 106496 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000875   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 108000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038206197 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.949       |\n",
      "|    explained_variance   | -0.00943    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    std                  | 0.088       |\n",
      "|    value_loss           | 0.000136    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000994 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 110000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000939 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 112000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00107 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 703    |\n",
      "|    iterations      | 14     |\n",
      "|    time_elapsed    | 163    |\n",
      "|    total_timesteps | 114688 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -5.36e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 116000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025576802 |\n",
      "|    clip_fraction        | 0.0951      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.11        |\n",
      "|    explained_variance   | -0.0038     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0336     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.0755      |\n",
      "|    value_loss           | 0.000156    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -8.64e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 118000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000149 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 120000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00021 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 701    |\n",
      "|    iterations      | 15     |\n",
      "|    time_elapsed    | 175    |\n",
      "|    total_timesteps | 122880 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000676   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 124000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027247578 |\n",
      "|    clip_fraction        | 0.0975      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.27        |\n",
      "|    explained_variance   | -0.000296   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    std                  | 0.0644      |\n",
      "|    value_loss           | 8.18e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00068 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000686 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 128000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000676 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 130000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 703    |\n",
      "|    iterations      | 16     |\n",
      "|    time_elapsed    | 186    |\n",
      "|    total_timesteps | 131072 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000776   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 132000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022289202 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.42        |\n",
      "|    explained_variance   | 0.00133     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0353     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.0559      |\n",
      "|    value_loss           | 5.55e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000463 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 134000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000653 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 136000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000489 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 138000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 704    |\n",
      "|    iterations      | 17     |\n",
      "|    time_elapsed    | 197    |\n",
      "|    total_timesteps | 139264 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 999        |\n",
      "|    mean_reward          | -0.00111   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 140000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01975891 |\n",
      "|    clip_fraction        | 0.0803     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.56       |\n",
      "|    explained_variance   | -0.000915  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0145     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.0488     |\n",
      "|    value_loss           | 3.46e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.0017  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 142000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.0011  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 144000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00127 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 146000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 704    |\n",
      "|    iterations      | 18     |\n",
      "|    time_elapsed    | 209    |\n",
      "|    total_timesteps | 147456 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 999       |\n",
      "|    mean_reward          | -0.000179 |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 148000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0164096 |\n",
      "|    clip_fraction        | 0.0685    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 1.68      |\n",
      "|    explained_variance   | -0.00795  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0258   |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.0122   |\n",
      "|    std                  | 0.0431    |\n",
      "|    value_loss           | 3.52e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000175 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 150000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000106 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 152000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000256 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 154000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 701    |\n",
      "|    iterations      | 19     |\n",
      "|    time_elapsed    | 221    |\n",
      "|    total_timesteps | 155648 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -5.96e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019833032 |\n",
      "|    clip_fraction        | 0.0743      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.81        |\n",
      "|    explained_variance   | -0.00856    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0534     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.0378      |\n",
      "|    value_loss           | 2.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -4.88e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 158000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000126 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 160000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -4.66e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 162000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 695    |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 235    |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -8.6e-05    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 164000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015802108 |\n",
      "|    clip_fraction        | 0.0637      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.93        |\n",
      "|    explained_variance   | -0.00969    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00673    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.0336      |\n",
      "|    value_loss           | 2.49e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -8.54e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 166000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -8.62e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 168000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -8.5e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -8.5e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 688    |\n",
      "|    iterations      | 21     |\n",
      "|    time_elapsed    | 249    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -9.8e-06    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 174000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013028663 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.04        |\n",
      "|    explained_variance   | -2.03e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00952    |\n",
      "|    std                  | 0.0301      |\n",
      "|    value_loss           | 1.33e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=176000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -8e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=178000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -8.2e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.02e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 180000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 684    |\n",
      "|    iterations      | 22     |\n",
      "|    time_elapsed    | 263    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 999          |\n",
      "|    mean_reward          | -0.000424    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 182000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128110405 |\n",
      "|    clip_fraction        | 0.0577       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.15         |\n",
      "|    explained_variance   | -0.00589     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00406      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00873     |\n",
      "|    std                  | 0.027        |\n",
      "|    value_loss           | 7.31e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000424 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 184000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000424 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 186000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000424 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 188000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 682    |\n",
      "|    iterations      | 23     |\n",
      "|    time_elapsed    | 276    |\n",
      "|    total_timesteps | 188416 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000576   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012129359 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.25        |\n",
      "|    explained_variance   | -0.00893    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0219     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00737    |\n",
      "|    std                  | 0.0247      |\n",
      "|    value_loss           | 1.03e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000579 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 192000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000582 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 194000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000579 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 196000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 680    |\n",
      "|    iterations      | 24     |\n",
      "|    time_elapsed    | 289    |\n",
      "|    total_timesteps | 196608 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000182   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 198000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011405715 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.34        |\n",
      "|    explained_variance   | -0.00977    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00865    |\n",
      "|    std                  | 0.0223      |\n",
      "|    value_loss           | 9.54e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000174 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 200000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000162 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 202000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000178 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 204000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 679    |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 301    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 999        |\n",
      "|    mean_reward          | -0.000152  |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 206000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00933765 |\n",
      "|    clip_fraction        | 0.0513     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 2.44       |\n",
      "|    explained_variance   | 0.000629   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.00729   |\n",
      "|    std                  | 0.0202     |\n",
      "|    value_loss           | 7.08e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000112 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 208000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000119 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 210000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000151 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 212000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 677    |\n",
      "|    iterations      | 26     |\n",
      "|    time_elapsed    | 314    |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 999          |\n",
      "|    mean_reward          | -6.7e-05     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 214000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077499254 |\n",
      "|    clip_fraction        | 0.0505       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.54         |\n",
      "|    explained_variance   | -0.00323     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0324      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00736     |\n",
      "|    std                  | 0.0186       |\n",
      "|    value_loss           | 1.05e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -6.02e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 216000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -6.04e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 218000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -7.06e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 220000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 678    |\n",
      "|    iterations      | 27     |\n",
      "|    time_elapsed    | 326    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000398   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 222000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011480028 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.62        |\n",
      "|    explained_variance   | -5.7e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00397    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00764    |\n",
      "|    std                  | 0.017       |\n",
      "|    value_loss           | 8.9e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000403 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 224000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000401 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 226000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000406 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 228000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 678    |\n",
      "|    iterations      | 28     |\n",
      "|    time_elapsed    | 338    |\n",
      "|    total_timesteps | 229376 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -1.66e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 230000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010476444 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.71        |\n",
      "|    explained_variance   | 0.000599    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    std                  | 0.0156      |\n",
      "|    value_loss           | 6.63e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.54e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 232000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.7e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 234000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.78e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 236000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 677    |\n",
      "|    iterations      | 29     |\n",
      "|    time_elapsed    | 350    |\n",
      "|    total_timesteps | 237568 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -5.6e-06    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 238000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013706455 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 2.8         |\n",
      "|    explained_variance   | -0.00398    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0057      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00853    |\n",
      "|    std                  | 0.0141      |\n",
      "|    value_loss           | 8.77e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -7.4e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -7.8e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 242000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -6.6e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 244000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 675    |\n",
      "|    iterations      | 30     |\n",
      "|    time_elapsed    | 363    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 999          |\n",
      "|    mean_reward          | -0.000248    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 246000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086970255 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.9          |\n",
      "|    explained_variance   | 0.00799      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0214      |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00765     |\n",
      "|    std                  | 0.0128       |\n",
      "|    value_loss           | 2.68e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000248 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 248000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000248 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 250000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000248 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 252000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 676    |\n",
      "|    iterations      | 31     |\n",
      "|    time_elapsed    | 375    |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 999          |\n",
      "|    mean_reward          | -0.000205    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 254000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043341876 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 2.95         |\n",
      "|    explained_variance   | -0.185       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00253      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000661    |\n",
      "|    std                  | 0.0124       |\n",
      "|    value_loss           | 1.01e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000206 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 256000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000206 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 258000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000206 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 260000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000206 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 262000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 675    |\n",
      "|    iterations      | 32     |\n",
      "|    time_elapsed    | 388    |\n",
      "|    total_timesteps | 262144 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 999        |\n",
      "|    mean_reward          | -4.5e-05   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 264000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00934174 |\n",
      "|    clip_fraction        | 0.0655     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.02       |\n",
      "|    explained_variance   | -0.055     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.000161  |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.00703   |\n",
      "|    std                  | 0.0115     |\n",
      "|    value_loss           | 2.81e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -4.5e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -4.5e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -4.5e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 675    |\n",
      "|    iterations      | 33     |\n",
      "|    time_elapsed    | 400    |\n",
      "|    total_timesteps | 270336 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -3e-06      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 272000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012069553 |\n",
      "|    clip_fraction        | 0.0586      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.1         |\n",
      "|    explained_variance   | -0.00803    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00722    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00697    |\n",
      "|    std                  | 0.0106      |\n",
      "|    value_loss           | 1.41e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=274000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -3e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -3e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -3e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 675    |\n",
      "|    iterations      | 34     |\n",
      "|    time_elapsed    | 412    |\n",
      "|    total_timesteps | 278528 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -5.5e-05    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006808401 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.18        |\n",
      "|    explained_variance   | -0.0168     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.009       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00507    |\n",
      "|    std                  | 0.00979     |\n",
      "|    value_loss           | 9.14e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -5.5e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -5.5e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -5.5e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 677    |\n",
      "|    iterations      | 35     |\n",
      "|    time_elapsed    | 423    |\n",
      "|    total_timesteps | 286720 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -1e-06      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 288000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008833181 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.25        |\n",
      "|    explained_variance   | -0.0647     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00365    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00491    |\n",
      "|    std                  | 0.00915     |\n",
      "|    value_loss           | 7.45e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=290000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 677    |\n",
      "|    iterations      | 36     |\n",
      "|    time_elapsed    | 435    |\n",
      "|    total_timesteps | 294912 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -6.48e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 296000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009630747 |\n",
      "|    clip_fraction        | 0.0513      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.33        |\n",
      "|    explained_variance   | 0.0192      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00687     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00666    |\n",
      "|    std                  | 0.00841     |\n",
      "|    value_loss           | 8.47e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -6.54e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 298000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -6.52e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 300000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -6.48e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 302000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 679    |\n",
      "|    iterations      | 37     |\n",
      "|    time_elapsed    | 446    |\n",
      "|    total_timesteps | 303104 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 999          |\n",
      "|    mean_reward          | -0.000339    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 304000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093115335 |\n",
      "|    clip_fraction        | 0.0711       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.41         |\n",
      "|    explained_variance   | -0.0618      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0176      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    std                  | 0.00779      |\n",
      "|    value_loss           | 4.91e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00034 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 306000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000339 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 308000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -0.00034 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 680    |\n",
      "|    iterations      | 38     |\n",
      "|    time_elapsed    | 457    |\n",
      "|    total_timesteps | 311296 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000306   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 312000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010275649 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.47        |\n",
      "|    explained_variance   | -0.0143     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00479     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00663    |\n",
      "|    std                  | 0.00728     |\n",
      "|    value_loss           | 3.55e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000306 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 314000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000306 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 316000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000305 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 318000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 680    |\n",
      "|    iterations      | 39     |\n",
      "|    time_elapsed    | 469    |\n",
      "|    total_timesteps | 319488 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -0.000129   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006634267 |\n",
      "|    clip_fraction        | 0.0635      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.54        |\n",
      "|    explained_variance   | 0.0068      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.007       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00562    |\n",
      "|    std                  | 0.00678     |\n",
      "|    value_loss           | 1.01e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000128 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 322000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000128 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 324000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -0.000129 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 326000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 679    |\n",
      "|    iterations      | 40     |\n",
      "|    time_elapsed    | 482    |\n",
      "|    total_timesteps | 327680 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=328000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -1.16e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 328000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012176469 |\n",
      "|    clip_fraction        | 0.0985      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.62        |\n",
      "|    explained_variance   | -0.0135     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0255     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00696    |\n",
      "|    std                  | 0.00632     |\n",
      "|    value_loss           | 1.01e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.18e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 330000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.16e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 332000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.2e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 334000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 679    |\n",
      "|    iterations      | 41     |\n",
      "|    time_elapsed    | 494    |\n",
      "|    total_timesteps | 335872 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 999          |\n",
      "|    mean_reward          | -6.6e-06     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 336000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077758906 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.69         |\n",
      "|    explained_variance   | 0.0109       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.012       |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00461     |\n",
      "|    std                  | 0.00589      |\n",
      "|    value_loss           | 9.76e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -7.8e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 338000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -6.6e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -6.8e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -7.4e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 676    |\n",
      "|    iterations      | 42     |\n",
      "|    time_elapsed    | 508    |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 999       |\n",
      "|    mean_reward          | -1.9e-05  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 346000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0100051 |\n",
      "|    clip_fraction        | 0.0702    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 3.76      |\n",
      "|    explained_variance   | -0.00487  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0125    |\n",
      "|    n_updates            | 420       |\n",
      "|    policy_gradient_loss | -0.00457  |\n",
      "|    std                  | 0.00553   |\n",
      "|    value_loss           | 1.78e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.9e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.9e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.9e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 676    |\n",
      "|    iterations      | 43     |\n",
      "|    time_elapsed    | 520    |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 999        |\n",
      "|    mean_reward          | -1.2e-06   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 354000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01000521 |\n",
      "|    clip_fraction        | 0.0436     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 3.79       |\n",
      "|    explained_variance   | -1.36      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | 0.00181    |\n",
      "|    std                  | 0.00544    |\n",
      "|    value_loss           | 7.04e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1.4e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 358000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -1e-06   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 678    |\n",
      "|    iterations      | 44     |\n",
      "|    time_elapsed    | 531    |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=362000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -3.1e-05    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 362000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004646343 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.81        |\n",
      "|    explained_variance   | -0.145      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.000692    |\n",
      "|    std                  | 0.00529     |\n",
      "|    value_loss           | 8.95e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -3.1e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -3.1e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -3.1e-05 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 676    |\n",
      "|    iterations      | 45     |\n",
      "|    time_elapsed    | 544    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -1.08e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 370000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008552834 |\n",
      "|    clip_fraction        | 0.0888      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.85        |\n",
      "|    explained_variance   | -0.0685     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00448    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.000298   |\n",
      "|    std                  | 0.00505     |\n",
      "|    value_loss           | 3.49e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.12e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 372000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.12e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 374000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.06e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 376000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 676    |\n",
      "|    iterations      | 46     |\n",
      "|    time_elapsed    | 556    |\n",
      "|    total_timesteps | 376832 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 999          |\n",
      "|    mean_reward          | -4e-06       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 378000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051595476 |\n",
      "|    clip_fraction        | 0.0704       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 3.88         |\n",
      "|    explained_variance   | -0.381       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00908     |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | 0.000663     |\n",
      "|    std                  | 0.00495      |\n",
      "|    value_loss           | 1.03e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -8.4e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.48e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 382000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=384000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.18e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 384000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 676    |\n",
      "|    iterations      | 47     |\n",
      "|    time_elapsed    | 569    |\n",
      "|    total_timesteps | 385024 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -1.74e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 386000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006893779 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.91        |\n",
      "|    explained_variance   | -0.227      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0282     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.000845    |\n",
      "|    std                  | 0.0048      |\n",
      "|    value_loss           | 3.48e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.44e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 388000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.56e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 390000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.14e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 392000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 675    |\n",
      "|    iterations      | 48     |\n",
      "|    time_elapsed    | 581    |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 999         |\n",
      "|    mean_reward          | -1.08e-05   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 394000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008098637 |\n",
      "|    clip_fraction        | 0.0775      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 3.95        |\n",
      "|    explained_variance   | -0.0872     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00794     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.000643   |\n",
      "|    std                  | 0.00461     |\n",
      "|    value_loss           | 3.18e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | -9.6e-06 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 396000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.26e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 398000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 999.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 999       |\n",
      "|    mean_reward     | -1.36e-05 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 400000    |\n",
      "----------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 675    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 594    |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Parallel environments\n",
    "train_env = make_vec_env(env_id, n_envs=NUM_CPU)\n",
    "eval_callback = EvalCallback(train_env, best_model_save_path=\"./logs/\",\n",
    "                             log_path=\"./logs/\", eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", train_env, verbose=1, tensorboard_log=\"./logs/ppo_MountainCar_tensorboard/\")\n",
    "model.learn(total_timesteps=1_000_000,\n",
    "            gae_lambda=0.98,\n",
    "            gamma=0.99,\n",
    "            n_epochs=4,\n",
    "            n_steps=NUM_CPU,\n",
    "            callback=eval_callback)\n",
    "model.save(\"ppo_MountainCar\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = make_vec_env(env_id, n_envs=1)\n",
    "\n",
    "model = PPO.load(\"ppo_MountainCar\")\n",
    "\n",
    "obs = test_env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = test_env.step(action)\n",
    "    test_env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "rl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
