{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import gymnasium as gym\n",
    "# sys.modules[\"gym\"] = gym\n",
    "import gym\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gnwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"MountainCarContinuous-v0\"\n",
    "NUM_CPU = 32  # Number of processes to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parallel environments\n",
    "train_env_f = make_vec_env(env_id, n_envs=NUM_CPU)\n",
    "train_env_f.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(train_env_f, \n",
    "                             best_model_save_path=\"./logs/best_model/expert\",\n",
    "                             log_path=\"./logs/results\",\n",
    "                             eval_freq=2**10/NUM_CPU,\n",
    "                             deterministic=True, render=False)\n",
    "model = SAC(\"MlpPolicy\", \n",
    "            train_env_f,\n",
    "            verbose=1, \n",
    "            seed=0,\n",
    "            learning_rate=0.0003,\n",
    "            buffer_size = 50000,\n",
    "            learning_starts = 0,\n",
    "            batch_size=512,\n",
    "            tau = 0.01,\n",
    "            gamma=0.9999,\n",
    "            gradient_steps=2*NUM_CPU,\n",
    "            use_sde=True,            \n",
    "            tensorboard_log=\"./logs/sac_MountainCar_tensorboard_expert/\")\n",
    "\n",
    "model.learn(total_timesteps=50_000, callback=eval_callback, progress_bar=True)\n",
    "model.save(\"./logs/sac_MountainCar_expert\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel environments\n",
    "train_env = make_vec_env(env_id, \n",
    "                         env_kwargs=dict(\n",
    "                            full_obs=False\n",
    "                         ),\n",
    "                         n_envs=NUM_CPU)\n",
    "train_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(train_env, \n",
    "                             best_model_save_path=\"./logs/best_model/expert_noob\",\n",
    "                             log_path=\"./logs/results\",\n",
    "                             eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            train_env,\n",
    "            verbose=1, \n",
    "            seed=0,\n",
    "            batch_size=512,\n",
    "            # ent_coef=0.00429,\n",
    "            learning_rate=7.77e-05,\n",
    "            n_epochs=10,\n",
    "            n_steps=8*NUM_CPU,\n",
    "            gae_lambda=0.9,\n",
    "            gamma=0.9999,\n",
    "            # clip_range=0.1,\n",
    "            max_grad_norm=5,\n",
    "            # vf_coef=0.19,\n",
    "            use_sde=True,\n",
    "            # policy_kwargs=dict(log_std_init=-3.29, ortho_init=False),\n",
    "            tensorboard_log=\"./logs/ppo_MountainCar_tensorboard_noob/\")\n",
    "\n",
    "model.learn(total_timesteps=1_000_000, callback=eval_callback, progress_bar=True)\n",
    "model.save(\"./logs/ppo_MountainCar_noob\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_env = gym.make(env_id, render_mode=\"human\")\n",
    "# test_env = gnwrapper.Animation(test_env)\n",
    "# test_env = DummyVecEnv([lambda: test_env])\n",
    "\n",
    "test_env = make_vec_env(env_id,\n",
    "                        # env_kwargs=dict(\n",
    "                        #     render_mode=\"rgb_array\"\n",
    "                        # ),\n",
    "                        wrapper_class=gnwrapper.Animation,\n",
    "                        n_envs=1)\n",
    "\n",
    "expert = PPO.load(\"./logs/ppo_MountainCar_expert.zip\", print_system_info=True)\n",
    "\n",
    "obs = test_env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = expert.predict(obs)\n",
    "    obs, rewards, dones, info = test_env.step(action)\n",
    "    test_env.render(mode=\"rgb_array\")\n",
    "\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(expert, test_env, 100)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "import dataclasses\n",
    "\n",
    "NUM_EPISODES = 10\n",
    "full_env = gym.make(env_id)\n",
    "rng = np.random.default_rng()\n",
    "rollouts = rollout.rollout(\n",
    "    expert,\n",
    "    DummyVecEnv([lambda: RolloutInfoWrapper(full_env)]),\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=100),\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "rollouts = [dataclasses.replace(rollout, obs=np.delete(rollout.obs, 2, 1)) for rollout in rollouts]\n",
    "transitions = rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"The `rollout` function generated a list of {len(rollouts)} {type(rollouts[0])}.\n",
    "After flattening, this list is turned into a {type(transitions)} object containing {len(transitions)} transitions.\n",
    "The transitions object contains arrays for: {', '.join(transitions.__dict__.keys())}.\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "env = gym.make(env_id, full_obs=False)\n",
    "env = gnwrapper.Animation(env)\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    "    # policy=ActorCriticPolicy(observation_space=env.observation_space,\n",
    "    #                          action_space=env.action_space,\n",
    "    #                          lr_schedule=lambda _: torch.finfo(torch.float32).max,\n",
    "    #                          net_arch=[64, 64]\n",
    "    #                          )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bc_trainer.train(n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noob.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reward, _ = evaluate_policy(bc_trainer.policy, env, 100)\n",
    "print(f\"BC reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noob = PPO.load(\"./logs/ppo_MountainCar_noob.zip\")\n",
    "reward, _ = evaluate_policy(noob, env, 100)\n",
    "print(f\"PPO reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_f = gym.make(env_id)\n",
    "env_f = gnwrapper.Animation(env_f)\n",
    "env_f.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = PPO.load(\"./logs/ppo_MountainCar_expert.zip\")\n",
    "reward, _ = evaluate_policy(expert, env_f, 100)\n",
    "print(f\"Expert reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "rl-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
